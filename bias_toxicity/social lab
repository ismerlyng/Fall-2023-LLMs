Bias and Toxicity 9.23.2023

Bias, as defined by Merriam-Webster, is "an inclination of temperament or outlookâ€¦especially: a personal and sometimes unreasoned judgement." This definition of bias, particularly the words "unreasoned judgement," suggest that someone's biased outlook could be harmful towards someone. Now, let's compare bias to toxicity. Toxicity, on the other hand, is much more forthcoming about its harmfulness. Merriam-Webster defines it as "the quality, state, or relative degree of being poisonous." 

Large Language Models (LLMs) have both bias and toxicity from their immense amounts of training data. It's no surprise that LLMs would pick up our human "unreasoned judgements" and "poisonous" thoughts all over the Internet. Canonical research on LLMs has found that even the Internet itself is biased. It mainly captures data from younger users and people from developed countries due to issues like Internet access. The result? LLMs are not accounting - at least not sufficiently - for the data of those being marginalized. In other words, LLMs ultimately perpetuate dominant and stereotypical narratives about society, people, and culture. In this United States, this means worsening sexist, racist, and immigration bias and related toxic points of view. It doesn't take much to go online to one of these models and think it's confirming what you already believe. 

To make matters worse, the present bias and toxicity in LLMs only increases with the size of the model. Recently, researchers have learned that we can ask certain models to "self-correct" on their bias, which has yielded successful results. The optimal solution would be to train models to not be biased or toxic, but even researchers working on the "self-correction" point to this as being a larger, systemic issue - and, I agree. Other professionals, beyond engineers and artificial intelligence (AI) firms, need to be involved in making LLMs safer and more inclusive. Open AI put out a call back in 2019 for social scientists to help align AI to human values. But what are our human values, especially in our "us versus them" climate? Do we need to leverage the help of the government for some regulations, or would that just take too long? Maybe bringing it back to a more grassroots level could help too, even if it simply starts with raising more awareness among those being impacted the most. Populations being marginalized by LLMs may be too busy trying to survive, providing for themselves as well as for their families, to know and understand the full extent and repercussions of the bias and toxicity in LLMs - now and in the future. 
